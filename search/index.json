[{"content":"Github Repository: Multiple-Choice-Question-Generation\nAI-Powered Multiple Choice Question (MCQ) Generation Platform An intelligent educational platform for automated Multiple Choice Question (MCQ) generation using Retrieval-Augmented Generation (RAG) and Knowledge Graph technologies. The system leverages course materials to generate contextually relevant, high-quality MCQs for assessment and practice. Built with support for multiple AI models through LiteLLM and Neo4j knowledge graph for enhanced question generation.\nArchitecture This is a microservices-based application with the following components:\nServices Auth Service (port 3001): User authentication and authorization Generation Service (port 3005): MCQ generation using LLM models with RAG RAG Service (port 3011): Retrieval-Augmented Generation for context extraction Indexing Service (port 3006): Document processing and knowledge graph indexing Frontend (port 3000): React-based web interface for MCQ management Infrastructure LiteLLM (port 9510): Unified interface for multiple LLM providers (Gemini, Azure OpenAI, AWS Claude) Neo4j (port 17474/17687): Graph database for knowledge representation PostgreSQL (port 15432): Relational database for user and course data MinIO (port 9000/9001): Object storage for documents and media Ollama (port 11434): Local LLM hosting Multiple-Choice Question Generation Pipeline This pipeline involves several steps to generate high-quality MCQs as shown below:\nProject Structure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 KLTN/ ├── services/ # Microservices │ ├── auth/ # Authentication service │ ├── generation/ # MCQ generation service │ ├── indexing/ # Document indexing service │ └── rag/ # RAG context retrieval service ├── frontend/ # React frontend application ├── libs/ # Shared libraries │ ├── base/ # Base utilities │ ├── graph_db/ # Neo4j integration │ ├── lite_llm/ # LiteLLM wrapper │ ├── logger/ # Logging utilities │ ├── postgres_db/ # PostgreSQL integration │ └── storage/ # MinIO storage utilities ├── docker/ # Dockerfiles for services ├── database/ # Database initialization scripts ├── outputs/ # Analysis outputs and results ├── archive/ # Research and evaluation scripts ├── config.yaml # LiteLLM configuration └── docker-compose.yml # Docker orchestration Constructed Knowledge Graph The knowledge graph is built from course materials, capturing concepts, relationships, and entities. It enables context-aware MCQ generation by providing structured information for question formulation.\nGetting Started Prerequisites Docker \u0026amp; Docker Compose Python 3.13+ Node.js 18+ (for frontend development) UV package manager (for Python dependencies) Installation \u0026amp; Running Clone the repository\n1 2 git clone \u0026lt;repository-url\u0026gt; cd KLTN Start all services with Docker Compose\n1 docker-compose up -d Access the application\nFrontend: http://localhost:3000 Neo4j Browser: http://localhost:17474 MinIO Console: http://localhost:9001 LiteLLM API: http://localhost:9510 Development Setup Backend Services 1 2 3 4 5 6 7 8 9 # Install UV package manager pip install uv # Install dependencies uv sync # Run a specific service (example: RAG service) cd services/rag uv run python -m src.main Frontend 1 2 3 cd frontend npm install npm run dev Supported Courses The platform currently supports the following courses:\nint3405: Introduction to Artificial Intelligence dsa2025: Data Structures and Algorithms 2025 rl2025: Reinforcement Learning 2025 Features MCQ Generation Automated MCQ Creation: Generate multiple choice questions from course materials Context-Aware Questions: Leverage RAG to create questions based on specific topics and difficulty levels Knowledge Graph Enhanced: Use graph relationships to generate questions testing concept connections Quality Control: Validate questions for clarity, difficulty, and educational value Customizable: Control question difficulty, topic focus, and distractor quality Knowledge Graph Integration Automated extraction of concepts, relationships, and entities from course materials Graph-based retrieval for contextually relevant question generation Entity similarity analysis for creating meaningful distractors Knowledge graph effectiveness evaluation for question quality Multi-Model Support Gemini 2.5 Flash Azure OpenAI (GPT-4, GPT-4o-mini) AWS Claude Local models via Ollama RAG Pipeline for MCQ Context Document chunking and embedding for course materials Semantic search to identify relevant content for questions Context extraction for question and answer generation Knowledge graph enhanced retrieval for topic selection User Management Role-based access control (Teachers/Students) Course enrollment and management Authentication with JWT tokens Technologies Backend Python 3.13 with UV package manager FastAPI for REST APIs Neo4j for knowledge graph storage PostgreSQL for relational data LiteLLM for unified LLM interface Frontend React 18 with TypeScript Vite for build tooling TailwindCSS for styling React Query for data fetching Zustand for state management Infrastructure Docker \u0026amp; Docker Compose for containerization MinIO for object storage Nginx for serving frontend Evaluation \u0026amp; Analysis The archive/ directory contains research and evaluation scripts for MCQ quality assessment:\nanalyze_kg_effectiveness.py: Knowledge graph impact on question quality kg_evaluation.py: Quantitative KG metrics for MCQ generation entities_similarity.ipynb: Entity similarity computation for distractor creation qa_topic_comparison.py: MCQ topic coverage analysis baseline.ipynb: Comparison with baseline MCQ generation methods MCQ generation results are stored in outputs/ organized by model and course.\nAPI Documentation Once services are running, API documentation is available at:\nAuth Service: http://localhost:3001/docs Generation Service: http://localhost:3005/docs RAG Service: http://localhost:3011/docs Indexing Service: http://localhost:3006/docs ","date":"2025-12-15T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/multiple-choice-question-generation/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/multiple-choice-question-generation/","title":"Multiple Choice Question Generation using LLMs and Knowledge Graphs"},{"content":"Github Repository: Private\nGraph-Based Recommendation System Intelligent Chatbot Recommendation System powered by Graph Retrieval\nAn enterprise-grade recommendation system that leverages graph databases, semantic search, and conversational AI to provide context-aware product recommendations.\nTable of Contents Overview How It Works Architecture Key Technologies Recommendation Flow Features Overview The Sun Assistant Recommendation System is designed to provide intelligent product recommendations through natural language conversations. By combining Graph Retrieval, Semantic Search, and Conversational AI, the system delivers accurate, context-aware recommendations that understand user intent and product relationships.\nCore Capabilities Hybrid Search - Combines BM25 keyword matching with semantic embedding scores Graph Retrieval - Leverages Neo4j to discover product relationships and connections Conversational Interface - Natural language interaction through chatbot Intelligent Ranking - Multi-factor scoring including relevance, popularity, and context Context Awareness - Maintains conversation history for better recommendations How It Works The recommendation system follows a sophisticated multi-stage pipeline:\n1. Query Understanding The chatbot processes user queries using LLM (Large Language Models) to:\nExtract user intent and preferences Identify key entities and attributes Decompose complex questions into sub-queries Maintain conversation context 2. Multi-Source Retrieval The system retrieves candidate products from three complementary sources:\nVector Search (OpenSearch)\nSemantic similarity using embeddings Captures meaning beyond exact keywords Returns products with similar descriptions Keyword Search (BM25)\nTraditional text matching Effective for exact product names or specifications Handles specific attribute queries Graph Retrieval (Neo4j)\nDiscovers product relationships Finds similar or complementary products Traverses connections like \u0026ldquo;bought together\u0026rdquo;, \u0026ldquo;similar to\u0026rdquo;, \u0026ldquo;alternative for\u0026rdquo; Explores product hierarchies and categories 3. Ranking \u0026amp; Filtering Retrieved candidates are scored and filtered using:\nRelevance Score: Combination of BM25 and embedding similarity Graph Score: Relationship strength from Neo4j Business Rules: Availability, price range, categories Context Filtering: User preferences and conversation history 4. Answer Generation The chatbot synthesizes recommendations into natural language:\nExplains why products are recommended Highlights key features and benefits Provides comparison insights Asks clarifying questions when needed Architecture 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ┌─────────────────────────────────────────────────────────┐ │ User Query (NL) │ │ \u0026#34;Show me running shoes under $100\u0026#34; │ └────────────────────┬────────────────────────────────────┘ │ ▼ ┌──────────────────────┐ │ Chatbot Service │ │ (Query Processing) │ │ • Intent Extraction │ │ • Sub-questions │ │ • Context Manager │ └──────────┬───────────┘ │ ┌────────────┼────────────┐ │ │ │ ▼ ▼ ▼ ┌──────────┐ ┌──────────┐ ┌──────────┐ │OpenSearch│ │ Neo4j │ │Azure AI │ │ │ │ Graph │ │ Search │ │ Semantic │ │ Relation │ │ BM25 │ │ Vectors │ │ Traversal│ │ Keywords │ └────┬─────┘ └────┬─────┘ └────┬─────┘ │ │ │ └────────────┼────────────┘ │ ▼ ┌─────────────────────┐ │ Ranking Engine │ │ • Score Fusion │ │ • Filter Rules │ │ • Context Weight │ └──────────┬──────────┘ │ ▼ ┌─────────────────────┐ │ Answer Generator │ │ (LLM Synthesis) │ │ • Format Results │ │ • Add Explanations │ │ • Generate Response│ └──────────┬──────────┘ │ ▼ ┌─────────────────────┐ │ Natural Language │ │ Response │ └─────────────────────┘ Key Technologies Graph Database (Neo4j) Why Graph for Recommendations?\nGraph databases excel at representing and querying relationships:\nProduct Relationships: \u0026ldquo;similar_to\u0026rdquo;, \u0026ldquo;complementary\u0026rdquo;, \u0026ldquo;bought_together\u0026rdquo; Category Hierarchies: Navigate from specific items to broader categories User Preferences: Track user interests and interaction patterns Fast Traversals: Efficiently explore multi-hop connections Example Graph Query:\n1 2 3 4 5 6 // Find products similar to running shoes MATCH (p:Product {name: \u0026#34;Nike Air Max\u0026#34;})-[:SIMILAR_TO]-\u0026gt;(similar:Product) WHERE similar.price \u0026lt; 100 RETURN similar.name, similar.price, similar.rating ORDER BY similar.rating DESC LIMIT 5 Vector Search (OpenSearch) Stores product embeddings (768-dimensional vectors) Performs semantic similarity search Captures meaning beyond exact text matches Fast approximate nearest neighbor (ANN) search Hybrid Scoring Combines multiple signals for optimal results:\n$$ \\text{FinalScore} = \\alpha \\cdot \\text{BM25Score} + \\beta \\cdot \\text{EmbeddingScore} + \\gamma \\cdot \\text{GraphScore} $$Where:\n$\\alpha, \\beta, \\gamma$ are tunable weights BM25Score: Keyword relevance EmbeddingScore: Semantic similarity GraphScore: Relationship strength Recommendation Flow Example Conversation User: \u0026ldquo;I need running shoes for trail running\u0026rdquo;\nSystem Process:\nQuery Analysis\nIntent: Product search Category: Shoes → Running Shoes → Trail Running Constraints: None explicit Retrieval\nVector Search: Products with similar descriptions to \u0026ldquo;trail running shoes\u0026rdquo; BM25: Exact matches for \u0026ldquo;trail running\u0026rdquo; and \u0026ldquo;shoes\u0026rdquo; Graph: 1 2 3 Trail Running → requires → Trail Running Shoes Trail Running Shoes → similar_to → Hiking Shoes Trail Running Shoes → complementary → Running Socks Ranking\nTop 10 trail running shoes by combined score Filter by availability and ratings Boost popular brands Response\n1 2 3 4 5 6 7 8 9 10 11 I found 5 great trail running shoes for you: 1. Salomon Speedcross 5 ($130) ⭐ 4.8/5 - Excellent grip for technical trails - Waterproof design 2. Brooks Cascadia 16 ($140) ⭐ 4.7/5 - Comfortable for long distances - Good cushioning Would you like more details or have a specific price range? User: \u0026ldquo;Something under $100?\u0026rdquo;\nSystem Process:\nContext Update: Add price constraint \u0026lt; $100 Re-rank: Apply price filter to previous results Graph Expansion: Look for alternatives in lower price range Response: Updated recommendations within budget Features Intelligent Query Understanding Multi-turn conversation with context retention Intent classification and entity extraction Handles ambiguous queries with clarifying questions Understands synonyms and variations Multi-Modal Retrieval Semantic Search: Understands meaning and context Keyword Matching: Precise for specific terms Graph Traversal: Discovers related products through relationships Advanced Ranking Relevance: How well product matches query Popularity: User ratings and purchase frequency Freshness: Recent additions and trending items Diversity: Varied results to increase discovery Graph-Powered Insights \u0026ldquo;Customers who bought this also bought\u0026hellip;\u0026rdquo; \u0026ldquo;Similar products you might like\u0026rdquo; \u0026ldquo;Complete your set with\u0026hellip;\u0026rdquo; Product comparison and alternatives Personalization (Future) User preference learning Interaction history tracking Collaborative filtering via graph Benefits For Users Natural language interaction - no complex filters needed Discover products through relationships and connections Contextual recommendations that understand intent Explanations for why products are recommended For Business Better product discovery → higher conversion Relationship-based recommendations → cross-selling Understand product connections and gaps Scalable architecture for large catalogs Technical Advantages Flexible: Easy to add new relationship types Explainable: Graph paths show recommendation reasoning Scalable: Distributed search and graph processing Accurate: Hybrid approach combines best of multiple methods Performance Retrieval Speed Vector Search: \u0026lt; 50ms (top 100 candidates) Graph Traversal: \u0026lt; 100ms (2-3 hop queries) BM25 Search: \u0026lt; 30ms Total Recommendation Time: \u0026lt; 500ms Accuracy Metrics Precision@5: Relevance of top 5 results Recall@20: Coverage of relevant products MRR (Mean Reciprocal Rank): Position of first relevant result Graph Coverage: % of products connected in graph Technology Stack Component Technology Purpose Chatbot Engine LangGraph, FastAPI Conversation orchestration LLM Azure OpenAI, Claude Query understanding \u0026amp; generation Vector DB OpenSearch Semantic search Graph DB Neo4j 5.22 Relationship retrieval Search Azure AI Search BM25 keyword matching Backend Python 3.11+, FastAPI API services Embeddings Azure OpenAI Ada-002 Text to vectors Key Learnings Why Graph Retrieval? Traditional recommendation systems rely on:\nContent-based filtering (similar features) Collaborative filtering (similar users) Graph retrieval adds:\nExplicit relationship modeling Multi-hop reasoning Explainable connections Dynamic relationship discovery Hybrid is Better No single retrieval method is perfect:\nVectors capture semantics but miss exact matches Keywords find exact terms but miss meaning Graphs discover relationships but need initial candidates Combining all three provides the best results.\nBuilt by Sun Asterisk\nIntelligent recommendations through conversational AI and graph intelligence\n","date":"2025-12-01T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/recommendation-system/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/recommendation-system/","title":"Chatbot Recommendation System"},{"content":"Github Repository: Private\nMoMorph VSCode Extension Sun MoMorph* is an AI-powered VSCode extension that transforms Figma designs into production-ready code. Built on top of GitHub Copilot\u0026rsquo;s language models and custom AI agents, MoMorph automates the journey from design to implementation with pixel-perfect accuracy.\nKey Features Figma-to-Code Generation Design Integration: Direct integration with Figma files linked to your Git repository Visual Analysis: Download and analyze Figma frames with detailed style extraction Pixel-Perfect Output: Generate UI components matching exact design specifications Screenshot Comparison: Automated visual regression testing against Figma designs AI-Powered Workflows Specification Generation: Automatically create detailed feature specs from Figma designs Technical Planning: Generate implementation plans with architecture decisions Code Generation: Create Next.js components, NestJS backends, and database schemas Multi-Agent System: Specialized agents for specification, planning, and implementation Developer Tools Chat Participant: Interact with @momorph in GitHub Copilot Chat Language Model Tools: Custom tools for Figma integration, screenshot comparison, and MCP protocol MCP Support: Model Context Protocol integration for extensibility Tree View UI: Browse Figma files, frames, and project contexts directly in VSCode Supported Use Cases Next.js frontend development with React/TypeScript NestJS backend API generation Database schema design from Figma prototypes UI component testing and validation Project Structure This is a monorepo managed with Yarn workspaces and Turborepo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 morpheus-vscode/ ├── packages/ │ ├── core/ # Core functionality \u0026amp; AI workflows │ │ └── src/ │ │ ├── chat/ # LangGraph workflows \u0026amp; AI agents │ │ ├── figma/ # Figma API client │ │ ├── github/ # GitHub integration │ │ ├── mcp/ # Model Context Protocol │ │ └── morpheus/ # Morpheus AI client │ ├── vscode/ # VSCode extension implementation │ │ ├── src/ │ │ │ ├── chat/ # Chat participant \u0026amp; handlers │ │ │ ├── commands/ # VSCode commands │ │ │ ├── tree/ # Tree view providers │ │ │ └── views/ # Webview panels │ │ └── resources/ │ │ ├── copilot/ # AI agent definitions │ │ ├── icons/ # Extension icons │ │ └── prompts/ # System prompts │ └── ui/ # Svelte-based webview UI │ └── src/ │ ├── components/ # UI components │ └── pages/ # Webview pages ├── libs/ │ ├── ipc/ # Inter-process communication │ └── tree-sitter/ # Code parsing utilities ├── deploy/ # Docker deployment configs └── docs/ # Documentation Package Overview core: Business logic, AI workflows using LangGraph, and integrations (Figma, GitHub, Morpheus AI) vscode: VSCode extension host, activation, commands, tree views, and chat participant ui: Svelte-based webview interface for browsing Figma files and contexts libs/ipc: Type-safe IPC messaging between extension and webview libs/tree-sitter: Language parsing for code understanding Prerequisites Node.js: Latest LTS version recommended (v20+) VSCode: Version 1.105.0 or higher Yarn: 4.6.0 (automatically installed via packageManager field) GitHub Copilot: Active subscription required for AI features Figma Account: For design file integration Getting Started 1. Clone the Repository 1 2 git clone \u0026lt;repository-url\u0026gt; cd morpheus-vscode 2. Setup Registry Authentication Since this project uses private packages from GitHub Packages:\nAccept invitation to the npm packages repo (sun-asterisk-research/morpheus-npm-pkgs) Skip if you\u0026rsquo;re already a member of sun-asterisk-research Create a Personal Access Token with read:packages permission (Create Token) Login to registry: 1 yarn npm login --scope @sun-asterisk-research Username: Your GitHub username Password: Your personal access token 3. Install Dependencies 1 yarn install This will install all dependencies for all packages in the monorepo.\nDevelopment Workflow Available Scripts Run these commands from the root directory:\nCommand Description yarn dev Start development mode with watch (all packages) yarn build Build all packages for production yarn build:dev Build all packages in development mode yarn build:types Generate TypeScript declaration files yarn typecheck Run TypeScript type checking yarn lint Run ESLint on all packages yarn test Run tests across all packages yarn package Create VSIX package for extension distribution yarn pre-commit Format, lint, and typecheck before committing Working on the Extension 1. Start Development Server 1 yarn dev This starts watch mode for all packages. Changes will be automatically recompiled.\n2. Launch Extension in Debug Mode Choose one of these methods:\nMethod A: Press F5 (default debug shortcut)\nMethod B: Use Run \u0026amp; Debug panel\nOpen Run \u0026amp; Debug sidebar (Ctrl+Shift+D) Select \u0026ldquo;Run Extension (morpheus-vscode)\u0026rdquo; Click the green play button This opens a new Extension Development Host window with MoMorph loaded.\n3. Test the Extension In the Extension Development Host window:\nOpen Figma Tree View: Click the MoMorph icon in the Activity Bar Use Chat Participant: Open GitHub Copilot Chat and type: 1 @momorph Hi! What can you do? Test Figma Integration: Ensure your workspace is linked to a Figma file 4. Reload Changes Extension code changes (core or vscode):\nPress Ctrl+Shift+F5 in Extension Development Host Or use the \u0026ldquo;Restart\u0026rdquo; button in the debug toolbar See VSCode Debugging Docs Webview UI changes (ui package):\nClick on any tree item to reload the webview Or use the \u0026ldquo;Reload\u0026rdquo; button in the webview toolbar Developing Chat Features MoMorph uses LangGraph for AI workflows. Read the detailed guide:\nChat Development Documentation\nQuick Overview:\nWorkflows: Define multi-step AI processes in packages/core/src/chat/graphs/workflows/ Agents: Custom Copilot agents in packages/vscode/resources/copilot/agents/ Tools: Language model tools registered in packages/vscode/package.json Code Quality \u0026amp; Pre-commit Checks Before committing, run:\n1 yarn pre-commit This will:\nFormat code with Prettier Run ESLint checks Perform TypeScript type checking Project Configuration Files File Purpose .vscode/ VSCode settings and launch configurations .yarnrc.yml Yarn package manager configuration turbo.json Turborepo build pipeline and caching tsconfig.base.json Shared TypeScript configuration eslint.config.mjs ESLint rules and plugins package.json Workspace configuration and scripts Building and Packaging Create VSIX Package To create a distributable .vsix file:\nEnsure tests pass:\n1 yarn test Build the extension:\n1 yarn build Create VSIX package:\n1 yarn package The .vsix file will be generated in packages/vscode/dist/.\nInstalling the VSIX To install the extension from VSIX:\n1 code --install-extension packages/vscode/dist/vscode-momorph-*.vsix Or use VSCode UI:\nOpen Extensions view (Ctrl+Shift+X) Click ... (Views and More Actions) Select \u0026ldquo;Install from VSIX\u0026hellip;\u0026rdquo; Choose the generated .vsix file Testing The project uses VSCode Extension Test framework:\n1 yarn test This runs tests across all packages that have test suites configured.\nArchitecture Technology Stack Language: TypeScript Package Manager: Yarn 4 with workspaces Build System: Turborepo for monorepo orchestration Bundler: esbuild (extension) + Vite (UI) UI Framework: Svelte 5 with TypeScript AI Framework: LangGraph for multi-agent workflows Testing: VSCode Extension Test Runner Key Integrations GitHub Copilot: Base language models and chat participant API Figma API: Design file access and image rendering Model Context Protocol: Extensible tool system Morpheus AI: Custom code generation backend GitHub API: Repository and authentication management Communication Flow 1 2 3 4 5 6 7 8 9 10 VSCode Extension Host ↓ Chat Participant (@momorph) ↓ LangGraph Workflows (AI Agents) ↓ ├─→ Language Model (GitHub Copilot) ├─→ Tools (Figma, GitHub, File Operations) ├─→ MCP Servers └─→ Morpheus AI Backend ","date":"2025-10-01T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/agentic-coding-system/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/agentic-coding-system/","title":"Agentic Coding System"},{"content":"Github Repository: Private\nSun Assistant Always ready, always listening!\nAn enterprise-grade AI assistant platform with intelligent document management, conversational AI, and multi-source knowledge retrieval.\nFeatures • Architecture • Quick Start • Documentation\nTable of Contents Overview Features Architecture Services Libraries Quick Start Development Deployment Testing Database Migrations Overview Sun Assistant is a comprehensive AI-powered assistant platform designed for enterprise environments. It combines advanced natural language processing, intelligent document management, and multi-modal data retrieval to provide accurate, context-aware responses to user queries.\nKey Capabilities Intelligent Chatbot - AI-powered question answering with LangGraph state management Document Management - Automated document processing and indexing Multi-Source Search - OpenSearch, PostgreSQL, and Neo4j integration Slack Integration - Seamless workplace communication Graph Knowledge Base - Neo4j-powered relationship mapping Agentic Architecture - Autonomous task planning with MCP (Model Context Protocol) Multi-Language Support - Vietnamese, Japanese, and English Features Conversational AI Multi-turn conversation with context awareness Intelligent question decomposition Dynamic task planning and orchestration Human-in-the-loop intervention for complex queries Checkpoint-based state persistence Document Intelligence Automated document ingestion and processing Azure Document Intelligence integration Semantic embedding generation Multi-format support (PDF, DOCX, images) Vector and graph-based indexing Knowledge Retrieval Semantic search with OpenSearch Graph-based relationship traversal with Neo4j Hybrid search combining vector and keyword matching Context filtering and ranking Answer aggregation from multiple sources Integration \u0026amp; Deployment Docker-based microservices architecture RESTful APIs with FastAPI Prometheus metrics and health checks Slack bot integration Horizontal scalability Architecture Sun Assistant follows a microservices architecture with clear separation of concerns:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ┌─────────────────────────────────────────────────────────────┐ │ User Interface │ │ (Slack / REST API) │ └────────────────────┬────────────────────────────────────────┘ │ ┌───────────────┼───────────────┐ │ │ │ ┌────▼────┐ ┌────▼────┐ ┌────▼────────┐ │ Slack │ │ Chatbot │ │ Doc │ │ Service │ │ Service │ │ Manager │ └────┬────┘ └────┬────┘ └────┬────────┘ │ │ │ └──────────────┼──────────────┘ │ ┌──────────────┼──────────────┐ │ │ │ ┌────▼─────┐ ┌────▼────┐ ┌─────▼─────┐ │OpenSearch│ │ Neo4j │ │PostgreSQL │ │ (Vector)│ │ (Graph) │ │ (Relat.) │ └──────────┘ └─────────┘ └───────────┘ Technology Stack Backend Framework: Python 3.11+, FastAPI, LangGraph AI/ML: LiteLLM (multi-provider LLM), Azure OpenAI, Claude Search Engines: OpenSearch (vector search), Azure AI Search Databases: PostgreSQL 17, Neo4j 5.22 (graph) Message Queue: Slack SDK Infrastructure: Docker, Docker Compose Package Manager: UV (ultra-fast Python package manager) Services 1. Chatbot Service AI-powered conversational interface with intelligent question answering.\nKey Features:\nQuestion processing and sub-question generation Context retrieval from multiple sources Answer generation using multiple LLM providers Conversation management and history Prometheus metrics Endpoints:\nGET /v1/healthz - Health check POST /v1/chat - Chat interface GET /v1/metrics - Prometheus metrics Learn more →\n2. Document Manager Service Handles document upload, processing, and indexing.\nFeatures:\nDocument upload and validation Azure Document Intelligence processing Embedding generation and storage Multi-index management Document lifecycle management Operations:\nAdd/Update/Delete documents Get document status Batch processing 3. Slack Service Seamless integration with Slack workspace.\nFeatures:\nDirect message handling Channel monitoring Feedback collection (1-5 star ratings) Health monitoring Error handling and maintenance mode Health Checks:\nNeo4j connectivity RAG service availability Bot responsiveness 4. MCP Server Model Context Protocol server for advanced task orchestration.\nCapabilities:\nDynamic tool discovery Task planning and execution Context management Multi-step reasoning 5. Text2GraphQL Service Natural language to GraphQL query translation for Neo4j queries.\nLibraries The project includes reusable libraries in the libs/ directory:\nLibrary Purpose aws_claude AWS Bedrock Claude integration azure_ai_search Azure AI Search wrapper azure_document_intelligent Azure Document Intelligence client base Common utilities and base classes litellm Multi-provider LLM abstraction logger Centralized logging neograph Neo4j graph database client open_search OpenSearch integration pg PostgreSQL database client slack_client Slack SDK wrapper storage Cloud storage abstraction Quick Start Prerequisites Docker \u0026amp; Docker Compose Python 3.11+ UV package manager Git 1. Clone the Repository 1 2 git clone https://github.com/framgia/sun-assistant.git cd sun-assistant 2. Environment Configuration Create a .env file with required configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Azure Search AZURE_SEARCH__INDEX_NAME=documents_production # PostgreSQL POSTGRES__USERNAME=your_username POSTGRES__PASSWORD=your_password POSTGRES__DB=sun_assistant POSTGRES__HOST=postgres # Neo4j NEO4J__HOST=neo4j NEO4J__PORT=7687 NEO4J__USERNAME=neo4j NEO4J__PASSWORD=your_neo4j_password # Slack SLACK__SIGNING_SECRET=your_signing_secret SLACK__BOT_TOKEN=xoxb-your-bot-token SLACK__APP_TOKEN=xapp-your-app-token SLACK__BOT_ID=your_bot_id # LLM Services LITELLM_SERVICE_URL=http://litellm:4000 CHATBOT_SERVICE_URL=http://chatbot_service:8000 3. Build and Start Services 1 bash ./scripts/get_started.sh Or manually with Docker Compose:\n1 docker-compose up -d 4. Verify Installation Check service health:\n1 2 3 4 5 6 7 8 # Chatbot service curl http://localhost:8000/v1/healthz # Neo4j browser open http://localhost:17474 # PostgreSQL psql -h localhost -p 15432 -U your_username -d sun_assistant Development Set Up Development Environment Install UV Package Manager 1 curl -LsSf https://astral.sh/uv/install.sh | sh Install Dependencies 1 uv sync Set Up Pre-commit Hooks 1 2 3 4 5 6 7 8 # Install pre-commit pip install pre-commit # Add to git hooks pre-commit install # Run on all files pre-commit run --all-files Local Development Workflow 1 2 3 4 5 6 7 # Work on a specific service cd services/chatbot uv sync uv run chatbot # Run with auto-reload uv run uvicorn src.chatbot.main:app --reload Code Style This project uses:\nBlack for code formatting isort for import sorting flake8 for linting mypy for type checking Pre-commit hooks automatically format code before commits.\nTesting Unit Tests Test files must be in the tests/ directory of each service.\n1 2 3 4 5 6 7 8 9 # Run all tests in a service cd services/chatbot python -m unittest discover # Run specific test file python -m unittest tests.test_module_name # Run with pytest uv run pytest Test Conventions Test files start with test_ (e.g., test_chatbot.py) Tests organized by feature/module Use descriptive test names Include docstrings for complex tests Integration Tests 1 2 # Run integration tests with Docker docker-compose run --rm chatbot_service uv run pytest tests/integration/ Deployment Docker Deployment 1 2 3 4 5 6 7 8 # Production build docker-compose -f docker-compose.yml up -d # Check logs docker-compose logs -f chatbot_service # Scale services docker-compose up -d --scale chatbot_service=3 Deployment Scripts 1 2 3 4 5 # Deploy to staging bash ./scripts/deploy/staging.sh # Deploy to production bash ./scripts/deploy/host_prod.sh Health Monitoring All services expose health check endpoints:\nChatbot: http://localhost:8000/v1/healthz Slack: Health check via tracker.py (every 6h) Document Manager: http://localhost:8001/healthz Database Migrations Neo4j Migrations Installation 1 2 3 curl -LO https://github.com/michael-simons/neo4j-migrations/releases/download/2.0.3/neo4j-migrations-2.0.3-linux-x86_64.zip unzip neo4j-migrations-2.0.3-linux-x86_64.zip echo \u0026#34;NEO4J__MIGRATION_PATH=/path/to/neo4j-migrations-2.0.3-linux-x86_64/bin\u0026#34; \u0026gt;\u0026gt; .env Usage 1 2 3 4 5 6 7 8 9 10 11 # Check migration status python migrations.py info # Apply pending migrations python migrations.py migrate # Run specific migration python migrations.py run V001__migration_name.cypher # Clean migrations history python migrations.py clean false Migration File Convention 1 2 3 4 5 migrations/V\u0026lt;version\u0026gt;__\u0026lt;description\u0026gt;.cypher Examples: - V000__Add_Communities_Label.cypher - V1_0_2__AddChunkLabel.cypher Data Migration 1 2 3 4 5 6 7 8 # Stop Neo4j service docker-compose stop neo4j # Dump data bash ./scripts/utils/dump_data.sh # Restore data bash ./scripts/utils/restore_data.sh Built by Sun Asterisk\nBack to Top\n","date":"2025-08-31T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/corporate-chatbot/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/corporate-chatbot/","title":"Corporate Chatbot"},{"content":"Github Repository: SmartGOV\nSmartGov - Hệ thống Chatbot hỗ trợ Thủ tục Hành chính SmartGov là một hệ thống chatbot thông minh được thiết kế để hỗ trợ công dân trong việc thực hiện các thủ tục hành chính. Hệ thống sử dụng công nghệ AI hiện đại, Knowledge Graph và Retrieval-Augmented Generation (RAG) để cung cấp thông tin chính xác và hữu ích về các dịch vụ công.\nTính năng chính Chatbot thông minh Multi-Agent Architecture: Hệ thống agent đa tầng với routing thông minh Profile Agent: Tự động điền thông tin cá nhân vào biểu mẫu GeoAdmin Agent: Trả lời các câu hỏi về địa danh và đơn vị hành chính RAG Agent: Tìm kiếm và trả lời dựa trên knowledge base Speech-to-Text: Hỗ trợ nhập liệu bằng giọng nói Xử lý tài liệu Document Parsing: Xử lý file DOCX, PDF và chuyển đổi sang văn bản Intelligent Chunking: Chia nhỏ tài liệu thành các đoạn có ý nghĩa Knowledge Graph Building: Tạo đồ thị tri thức từ nội dung tài liệu Embedding: Vector hóa nội dung để tìm kiếm ngữ nghĩa Giao diện người dùng Modern React UI: Giao diện đẹp mắt với Tailwind CSS Real-time Chat: Trò chuyện thời gian thực với hiệu ứng typing Document Management: Quản lý và đánh chỉ mục tài liệu Responsive Design: Tương thích mọi thiết bị Kiến trúc hệ thống Microservices Architecture 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ Frontend │ │ Chatbot │ │ Indexing │ │ (React) │◄──►│ Service │◄──►│ Service │ │ Port: 5173 │ │ Port: 3010 │ │ Port: 3006 │ └─────────────────┘ └─────────────────┘ └─────────────────┘ │ │ │ │ │ │ ▼ ▼ ▼ ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ │ RAG Service │ │ STT Service │ │ LiteLL M │ │ Port: 8001 │ │ Port: 8000 │ │ Port: 9510 │ └─────────────────┘ └─────────────────┘ └─────────────────┘ │ │ │ └───────────────────────┼───────────────────────┘ │ ┌───────────────────────▼───────────────────────┐ │ Data Layer │ │ │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ │ │PostgreSQL│ │ Neo4j │ │ MinIO │ │ │ │Port:15432│ │Port:17474│ │Port: 9000│ │ │ └──────────┘ └──────────┘ └──────────┘ │ └───────────────────────────────────────────────┘ Services Overview Core Services Chatbot Service (services/chatbot/)\nOrchestrates conversation flow Implements multi-agent routing Handles user interactions and responses Indexing Service (services/indexing/)\nProcesses and indexes documents Builds knowledge graphs from content Manages document embeddings RAG Service (services/rag/)\nImplements Retrieval-Augmented Generation Semantic search in knowledge base Context-aware response generation STT Service (services/stt/)\nSpeech-to-Text conversion Voice input processing Audio transcription Library Modules Base (libs/base/)\nCommon base classes and utilities Shared Pydantic models Application foundations LiteLLM (libs/lite_llm/)\nLLM integration wrapper Multiple AI model support (Gemini, OpenAI, etc.) Unified API interface Graph DB (libs/graph_db/)\nNeo4j database interface Knowledge graph operations Cypher query abstractions Storage (libs/storage/)\nMinIO object storage integration File upload/download management Binary data handling Logger (libs/logger/)\nStructured logging system Centralized log management Debug and monitoring support PG (libs/pg/)\nPostgreSQL database interface SQL operations and migrations Data persistence layer Cài đặt và triển khai Yêu cầu hệ thống Python: \u0026gt;= 3.13 Node.js: \u0026gt;= 18.0.0 Docker \u0026amp; Docker Compose: Latest version Git: Latest version 1. Clone Repository 1 2 git clone https://github.com/vuniem131104/UET-Codecamp-2025.git cd UET-Codecamp-2025 2. Cấu hình Environment Variables Tạo file .env từ template:\n1 cp .env.example .env Cấu hình các biến môi trường:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Database Configuration POSTGRES__USER=admin POSTGRES__PASSWORD=secure_password POSTGRES__DB=smartgov_db POSTGRES__HOST=localhost POSTGRES__PORT=15432 # Neo4j Configuration NEO4J__USERNAME=neo4j NEO4J__PASSWORD=secure_neo4j_password # MinIO Configuration MINIO__ACCESS_KEY=minioadmin MINIO__SECRET_KEY=minioadmin123 # AI API Keys GEMINI_API_KEY1=your_gemini_api_key_1 GEMINI_API_KEY2=your_gemini_api_key_2 # ... add more API keys as needed 3. Khởi động Infrastructure 1 2 3 4 5 # Khởi động databases và dependencies docker-compose up -d postgres neo4j minio redis litellm_service # Kiểm tra trạng thái services docker-compose ps 4. Cài đặt Python Dependencies 1 2 3 4 5 # Sử dụng uv để quản lý dependencies pip install uv # Cài đặt workspace dependencies uv sync 5. Khởi động Backend Services 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Terminal 1: Chatbot Service cd services/chatbot uv run python -m chatbot # Terminal 2: Indexing Service cd services/indexing uv run python -m indexing # Terminal 3: RAG Service cd services/rag uv run python -m rag # Terminal 4: STT Service cd services/stt uv run python -m stt 6. Khởi động Frontend 1 2 3 cd frontend npm install npm run dev Hướng dẫn sử dụng 1. Truy cập ứng dụng Mở trình duyệt và truy cập: http://localhost:5173\n2. Upload và đánh chỉ mục tài liệu Click vào tab \u0026ldquo;Indexing\u0026rdquo; trong sidebar Upload file DOCX chứa thông tin thủ tục hành chính Click \u0026ldquo;Index\u0026rdquo; để xử lý tài liệu Chờ quá trình đánh chỉ mục hoàn thành 3. Sử dụng Chatbot Quay lại tab \u0026ldquo;Chatbot\u0026rdquo; Nhập câu hỏi về thủ tục hành chính Hoặc sử dụng microphone để nói Nhận câu trả lời từ AI agent 4. Các loại câu hỏi được hỗ trợ Câu hỏi chung về thủ tục 1 2 \u0026#34;Thủ tục cấp căn cước công dân cần những giấy tờ gì?\u0026#34; \u0026#34;Thời gian xử lý hồ sơ đăng ký xe máy là bao lâu?\u0026#34; Câu hỏi về địa danh, cơ quan 1 2 \u0026#34;Ủy ban nhân dân phường Cầu Giấy ở đâu?\u0026#34; \u0026#34;Thành phố Hà Nội có bao nhiều quận?\u0026#34; Yêu cầu điền form tự động 1 2 \u0026#34;Giúp tôi điền đơn xin cấp căn cước công dân\u0026#34; \u0026#34;Tạo đơn đăng ký tạm trú cho tôi\u0026#34; Phát triển Cấu trúc dự án 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 UET-CodeCamp-2025/ ├── services/ # Microservices │ ├── chatbot/ # Main chatbot logic │ ├── indexing/ # Document processing │ ├── rag/ # RAG implementation │ └── stt/ # Speech-to-text ├── libs/ # Shared libraries │ ├── base/ # Base classes │ ├── lite_llm/ # LLM integration │ ├── graph_db/ # Neo4j interface │ ├── storage/ # MinIO interface │ ├── logger/ # Logging system │ └── pg/ # PostgreSQL interface ├── frontend/ # React frontend ├── docker/ # Docker configurations ├── data/ # Database data ├── logs/ # Application logs └── config/ # Configuration files Thêm tính năng mới 1. Tạo Agent mới 1 2 3 4 5 6 7 8 9 10 11 # services/chatbot/src/chatbot/domain/new_agent/service.py from base import BaseService from chatbot.shared.state.chatbot_state import ChatbotState class NewAgentService(BaseService): def process(self, state: ChatbotState) -\u0026gt; dict: # Implement your agent logic here return { \u0026#34;answer\u0026#34;: \u0026#34;Response from new agent\u0026#34;, \u0026#34;references\u0026#34;: [] } 2. Đăng ký Agent trong Router 1 2 3 4 5 6 # services/chatbot/src/chatbot/application/chatbot.py def route_agent(self, state: ChatbotState) -\u0026gt; Literal[\u0026#34;new_agent\u0026#34;, ...]: # Add routing logic for new agent if \u0026#34;new_feature\u0026#34; in state.get(\u0026#39;raw_question\u0026#39;, \u0026#39;\u0026#39;).lower(): return \u0026#34;new_agent\u0026#34; # ... existing routing logic 3. Thêm vào Graph 1 2 3 4 5 6 7 # services/chatbot/src/chatbot/application/chatbot.py @property def nodes(self) -\u0026gt; dict[str, Any]: return { # ... existing nodes \u0026#34;new_agent\u0026#34;: self.new_agent.process, } Testing 1 2 3 4 5 6 7 8 9 # Run unit tests uv run pytest # Run integration tests uv run pytest tests/integration/ # Test specific service cd services/chatbot uv run pytest tests/ Code Quality 1 2 3 4 5 6 7 # Format code uv run black . uv run isort . # Lint code uv run flake8 uv run mypy . Monitoring và Logs Log Locations Application Logs: logs/ Neo4j Logs: logs/neo4j/ Container Logs: docker-compose logs [service] Health Checks 1 2 3 4 5 # Check service health curl http://localhost:3010/health # Chatbot curl http://localhost:3006/health # Indexing curl http://localhost:8001/health # RAG curl http://localhost:8000/health # STT Database Access 1 2 3 4 5 6 7 8 # PostgreSQL psql -h localhost -p 15432 -U admin -d smartgov_db # Neo4j Browser open http://localhost:17474 # MinIO Console open http://localhost:9001 Troubleshooting Các lỗi thường gặp 1. Service không khởi động được 1 2 3 4 5 # Kiểm tra logs docker-compose logs [service-name] # Restart service docker-compose restart [service-name] 2. Database connection failed 1 2 3 4 5 6 # Kiểm tra database status docker-compose ps # Reset database docker-compose down docker-compose up -d postgres neo4j 3. Frontend build errors 1 2 3 4 5 # Clear cache và reinstall cd frontend rm -rf node_modules package-lock.json npm install npm run dev 4. Python import errors 1 2 3 4 5 # Reinstall dependencies uv sync --force # Check Python path uv run python -c \u0026#34;import sys; print(sys.path)\u0026#34; Đóng góp Quy trình đóng góp Fork repository Tạo feature branch: git checkout -b feature/amazing-feature Commit changes: git commit -m 'Add amazing feature' Push to branch: git push origin feature/amazing-feature Tạo Pull Request Coding Standards Python: Tuân thủ PEP 8, sử dụng type hints TypeScript: Tuân thủ ESLint config Documentation: Viết docstring cho functions/classes Testing: Đảm bảo coverage \u0026gt;= 80% Commit Convention 1 2 3 4 5 type(scope): subject body footer Ví dụ:\n1 2 3 4 5 6 7 feat(chatbot): add new profile agent - Implement automatic form filling - Add user profile detection - Update routing logic Closes #123 License Dự án này được phát hành dưới MIT License. Xem file LICENSE để biết thêm chi tiết.\nTác giả Lê Hoàng Vũ - Lead Developer - @vulh-1357 UET-VNU Team - Contributors Acknowledgments Đại học Công nghệ - ĐHQGHN (UET-VNU) Google Gemini AI Platform Neo4j Graph Database FastAPI Framework React \u0026amp; TypeScript Community Liên hệ Email: le.hoang.vu@sun-asterisk.com GitHub: https://github.com/vulh-1357/UET-Codecamp-2025 Website: https://uet.vnu.edu.vn SmartGov - Democratizing access to government services through AI\n","date":"2025-08-17T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/smartgov/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/smartgov/","title":"SmartGOV - Chatbot for Administrative Procedures"},{"content":"Github Repository: Flowers-Colorization\nFlowers-Colorization using DDPM This project applies Denoising Diffusion Probabilistic Models (DDPM) to the task of automatic colorization of flower images with MLOps Pipeline.\nBy learning a diffusion-based generative process, the model can transform grayscale (black-and-white) flower images into realistic and vivid colorized versions.\nProject Overview Goal:\nTo develop a model that takes a grayscale image of a flower and generates a realistic colorized image.\nApproach:\nWe employ a DDPM architecture that learns to reverse a gradual noising process, starting from a noise distribution to generate a colorful, detailed image conditioned on the input grayscale image.\nDataset:\nWe use a dataset of flower images on kaggle (https://www.kaggle.com/datasets/imsparsh/flowers-dataset), preprocessing it.\nProject Structure 1 2 3 4 5 6 7 Flowers-Colorization/ │ ├── .github/ # Github action ├── config/ # Config for MLOps pipeline ├── src/ # All the souce code ├── requirements.txt # Python dependencies └── README.md # Project description (you are here!) Model Details Backbone:\nA U-Net model is used for the noise prediction network within the DDPM framework.\nConditioning:\nThe model conditions on grayscale images during both training and sampling.\nTraining Objective:\nMinimize the mean squared error (MSE) between the predicted noise and the true noise at different timesteps.\nQuick Start 1. Install Dependencies 1 pip install -r requirements.txt You can modify the data path in config.py accordingly.\n2. Run Full Pipeline 1 python main.py Training progress (losses, sample generations) will be logged for monitoring.\n3. Generate Colorized Flowers After training:\n1 python -m src.utils.inference This will produce colorized versions of the grayscale flower images.\nMain Features Flexible: Easily adapt to different flower datasets or resolution sizes. Configurable: Hyperparameters like noise schedule, number of diffusion steps, learning rate, batch size are adjustable via config.py. Visualization: Intermediate colorizations during sampling are saved for analysis. Results Sample colorized outputs:\nGrayscale Input Colorized Output Requirements Python \u0026gt;= 3.8 PyTorch \u0026gt;= 1.10 torchvision numpy tqdm matplotlib (optional) wandb or tensorboard for experiment tracking See requirements.txt for full details.\nReferences DDPM Paper: Denoising Diffusion Probabilistic Models (Ho et al., 2020) U-Net Paper: U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015) ","date":"2025-04-29T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/flowers-colorization/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/flowers-colorization/","title":"Flowers Colorization using Denoising Diffusion Probabilistic Models"},{"content":"Github Repository: Resume-Evaluation-Based-On-LLMs\nResume-Evaluation-Based-On-LLMs A smart, asynchronous resume evaluation system powered by LLMs, Redis, and FastAPI. This system allows users to upload resumes, job descriptions and receive both general and personalized feedback through automated evaluation and virtual interviews.\nUser Interface 1. Main Users upload their resumes and job descriptions here Then, click \u0026ldquo;Evaluate Resume\u0026rdquo; button and receive result Virtual Interview Jobs recommedation based on users\u0026rsquo; history Users\u0026rsquo; evaluation history 🔧 Tech Stack FastAPI – API server Redis – Message queue and temporary result store Redis Queue – Asynchronous task handling (e.g., resume evaluation) Llama 4 – Used for text extraction, job/resume standardization and evaluate content and layout of the resume. Also, it will serve as a virtual recruiter and give personalized feedback OpenAI Whisper – Voice recognition for virtual interviews Tavily + Web Search Agent – Recent job recommendations MongoDB – User history and job storage, user database AWS S3 – Store resumes files uploaded by users System Architecture 1. Resume Evaluation Flow Step 1: Users upload their resume and provide the job description. Step 2: The system asynchronously extracts and standardizes text from both the resume and the job description using LLMs. Step 3: The evaluation task is queued via Redis and executed by workers. Once complete, the user receives a non-personalized feedback. Optional: If the user wants more personalized insights, they proceed to a virtual interview. 2. Virtual Interview and Personalized Feedback Step 4: The LLM generates interview questions based on the job description. Step 5: The user replies via text or voice (processed by Whisper). Step 6: The conversation is passed to Qwen 2.5, which generates personalized feedback. Final: Feedback is returned to the user. 3. Job Recommendation via Web Search Agent Input: User submits a request to find relevant jobs. Process: Job preferences are fetched from MongoDB. Tavily-powered Web Search Agent fetches the latest related job postings. Output: Returned to the user. How It Works (Key Points) Uses Redis to enqueue/dequeue long-running tasks like resume evaluation. Separation between upload and evaluation actions to allow flexibility for users. Fully asynchronous, scalable design with pluggable LLMs. Feedback available in 2 modes: quick general or in-depth personalized. How to Run Locally 1. Clone Repo 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 1. Clone the repo git clone https://github.com/vuniem131104/Resume-Evaluation-Based-On-LLMs.git cd Resume-Evaluation-Based-On-LLMs/app # 2. Set up virtual environment and install dependencies python -m venv venv source venv/bin/activate pip install -r requirements.txt # 3. Start Redis server redis-server # 4. Create .env file similar to .env.example in this repo then put it into folder app/ # 5. Run the FastAPI app python main.py # 6. In another terminal, start the workers python resume_evaluation_worker.py # for resume evaluation worker python related_jobs_worker.py # for related jobs retrieval worker 2. Docker Make sure that you have installed docker and docker-compose in your local machine :))) Your folder structur will look like this: resume-application/\n├── uploads/ ├── .env └── docker-compose.yml\n1 2 3 4 5 6 7 8 9 10 # 1. Create a folder in your local machine mkdir resume-application \u0026amp;\u0026amp; cd resume-application mkdir uploads # 2. Copy docker-compose-user.yml in this repo into the folder # 3. Create .env file similar to .env.example in this repo # 4. Run Application docker-compose up -d Future Improvements Add OAuth2 login for real users Enable file storage via S3 or MinIO Add analytics dashboard for job fit trends Implement feedback history per user ","date":"2025-04-01T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/resume-evaluation/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/resume-evaluation/","title":"Resume Evaluation System"},{"content":"Github Repository: Super-Resolution-With-Pytorch\nOverview This repo is the implementation of SRRESNET and SRGAN in super resolution task for blurry images\nSRRESNET ARCHITECTURE The SRResNet is composed of the following operations –\nFirst, the low resolution image is convolved with a large kernel size 9x9 and a stride of 1, producing a feature map at the same resolution but with 64 channels. A parametric ReLU (PReLU) activation is applied. This feature map is passed through 16 residual blocks, each consisting of a convolution with a 3x3 kernel and a stride of 1, batch normalization and PReLU activation, another but similar convolution, and a second batch normalization. The resolution and number of channels are maintained in each convolutional layer. The result from the series of residual blocks is passed through a convolutional layer with a 3x3 kernel and a stride of 1, and batch normalized. The resolution and number of channels are maintained. In addition to the skip connections in each residual block (by definition), there is a larger skip connection arching across all residual blocks and this convolutional layer. 2 subpixel convolution blocks, each upscaling dimensions by a factor of 2 (followed by PReLU activation), produce a net 4x upscaling. The number of channels is maintained. Finally, a convolution with a large kernel size 9x9 and a stride of 1 is applied at this higher resolution, and the result is Tanh-activated to produce the super-resolved image with RGB channels in the range [-1, 1]. The SRResNet Update Training the SRResNet, like any network, is composed of a series of updates to its parameters. What might constitute such an update?\nOur training data will consist of high-resolution (gold) images, and their low-resolution counterparts which we create by 4x-downsampling them using bicubic interpolation.\nIn the forward pass, the SRResNet produces a super-resolved image at 4x the dimensions of the low-resolution image that was provided to it.\nWe use the Mean-Squared Error (MSE) as the loss function to compare the super-resolved image with this original, gold high-resolution image that was used to create the low-resolution image.\nChoosing to minimize the MSE between the super-resolved and gold images means we will change the parameters of the SRResNet in a way that, if given the low-resolution image again, it will create a super-resolved image that is closer in appearance to the original high-resolution version.\nThe MSE loss is a type of content loss, because it is based purely on the contents of the predicted and target images.\nIn this specific case, we are considering their contents in the RGB space – we will discuss the significance of this soon.\nSRGAN ARCHITECTURE It consists of a Generator and a Discriminator as other conventional GANS\nGENERATOR It will be the same as the SRRESNET, and we will take the pretrained SRRESNET to initialize for the GENERATOR\nDISCRIMINATOR As you might expect, the Discriminator is a convolutional network that functions as a binary image classifier.\nIt is composed of the following operations –\nThe high-resolution image (of natural or artificial origin) is convolved with a large kernel size $9\\times9$ and a stride of $1$, producing a feature map at the same resolution but with $64$ channels. A leaky ReLU activation is applied.\nThis feature map is passed through $7$ convolutional blocks, each consisting of a convolution with a $3\\times3$ kernel, batch normalization, and leaky ReLU activation. The number of channels is doubled in even-indexed blocks. Feature map dimensions are halved in odd-indexed blocks using a stride of $2$.\nThe result from this series of convolutional blocks is flattened and linearly transformed into a vector of size $1024$, followed by leaky ReLU activation.\nA final linear transformation yields a single logit, which can be converted into a probability score using the Sigmoid activation function. This indicates the probability of the original input being a natural (gold) image.\nGENERATOR UPDATE We will update the generator by using pretrained VGG19 model from torchvision. We no longer compare the orginial high resolution images with super resolution images but we compare their outputs after forward them through the truncated VGGV19. \\ Moreover, we utilize the advantage of an adversirial loss by using BCEWithLogitsLoss in pytorch to compare super resolution images passing through the discriminator and it untrue label (1). DISCRIMINATOR UPDATE It is very straightforward because it just distinguish between high resolution images with real labels (1) and super resolution images with it real labels (0). TRAINING MODELS Our models are trained on COCO2024 dataset. If you want to train on your dataset, please do the following steps:\nTrain SRRESNET 1 python3 train_srresnet.py --data_folder \u0026lt;your data directory\u0026gt; --batch_size \u0026lt;your batch size\u0026gt; --epochs \u0026lt;epochs to train models\u0026gt; Train SRGAN 1 python3 train_srgan.py --data_folder \u0026lt;your data directory\u0026gt; --batch_size \u0026lt;your batch size\u0026gt; --epochs \u0026lt;epochs to train models\u0026gt; TESTING MODELS Test SRRESNET with images 1 python3 test_srresnet.py --image_path \u0026lt;path to your image\u0026gt; --srresnet_ckpt \u0026lt;your srresnet checkpoint\u0026gt; Test SRGAN with images 1 python3 test_srgan.py --image_path \u0026lt;path to your image\u0026gt; --srgan_ckpt \u0026lt;your srgan checkpoint\u0026gt; RESULT ","date":"2025-02-01T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/image-super-resolution/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/image-super-resolution/","title":"Image Super-Resolution"},{"content":"Github Repository: License-Plate-Recognition\nSummary Use openCV and easyOCR to read text from license plate in a car detected by YOLO Train YOLO on a custom dataset to create model that can detect license plate easily (get the data here) The video I used in this project can be downloaded here. You can see the result below: Steps you should follow if you want to do the same: Clone this repo into your local computer and open it in your IDE Download the license plate detector model here Clone this repo and put it into the folder you have cloned Execute main.py to create output.csv, then add_missing_data to generate test_interpolated.csv To have a csv file named out2.csv, you should sort test_interpolated.csv by \u0026ldquo;frame_nmr\u0026rdquo; column and save in your local. Last, you run visualize.py to see the results. Good Luck! ","date":"2024-03-01T00:00:00Z","image":"https://vuniem131104.github.io/my-portfolio/p/license-plate-recognition/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://vuniem131104.github.io/my-portfolio/p/license-plate-recognition/","title":"License Plate Recognition"}]